"""Subject-level prediction benchmarks."""

import numpy as np
import pandas as pd
import torch
from typing import Dict, List, Optional, Tuple, Any, Union
from pathlib import Path
from sklearn.model_selection import StratifiedKFold, KFold
from sklearn.metrics import (
    mean_absolute_error, mean_squared_error, r2_score,
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score, confusion_matrix,
    classification_report
)
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr, spearmanr
from dataclasses import dataclass
import json
import time

from ..models.base import BaseConnectomeModel
from ..tasks.base import BaseConnectomeTask
from ..advanced_training import AdvancedConnectomeTrainer, TrainingConfig


@dataclass
class BenchmarkConfig:
    """Configuration for benchmarking experiments."""
    n_folds: int = 5
    test_size: float = 0.2
    random_state: int = 42
    stratify: bool = True
    metrics: List[str] = None
    save_predictions: bool = True
    save_models: bool = False
    verbose: bool = True


class SubjectBenchmark:
    """Comprehensive benchmarking suite for subject-level predictions.
    
    Supports multiple tasks:
    - Age prediction
    - Sex classification
    - Cognitive scores (7 different measures)
    - Clinical diagnosis (when available)
    """
    
    def __init__(
        self,
        config: Optional[BenchmarkConfig] = None,
        output_dir: Path = None
    ):
        self.config = config or BenchmarkConfig()
        self.output_dir = output_dir or Path("benchmark_results")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Default metrics per task type
        self.default_metrics = {
            'regression': ['mae', 'rmse', 'r2', 'pearson_r', 'spearman_r'],
            'binary_classification': ['accuracy', 'precision', 'recall', 'f1', 'auc', 'ap'],
            'multiclass_classification': ['accuracy', 'precision', 'recall', 'f1', 'macro_f1'],
        }
        
        # Results storage
        self.results = {}\n        self.detailed_results = {}\n    \n    def evaluate(\n        self,\n        model: BaseConnectomeModel,\n        dataset,\n        task: BaseConnectomeTask,\n        metrics: Optional[List[str]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Evaluate model on dataset with comprehensive metrics.\n        \n        Args:\n            model: Trained connectome model\n            dataset: Dataset to evaluate on\n            task: Prediction task\n            metrics: List of metrics to compute\n            \n        Returns:\n            Dictionary containing evaluation results\n        \"\"\"\n        if metrics is None:\n            metrics = self.default_metrics.get(task.task_type, ['mae', 'accuracy'])\n        \n        print(f\"Starting evaluation with {len(dataset)} samples\")\n        print(f\"Task: {task.get_task_info()}\")\n        print(f\"Metrics: {metrics}\")\n        \n        start_time = time.time()\n        \n        # Cross-validation evaluation\n        cv_results = self._cross_validate(\n            model, dataset, task, metrics\n        )\n        \n        # Holdout evaluation\n        holdout_results = self._holdout_evaluate(\n            model, dataset, task, metrics\n        )\n        \n        # Combine results\n        evaluation_results = {\n            'cross_validation': cv_results,\n            'holdout': holdout_results,\n            'task_info': task.get_task_info(),\n            'model_info': {\n                'name': model.__class__.__name__,\n                'parameters': sum(p.numel() for p in model.parameters()),\n            },\n            'dataset_info': {\n                'size': len(dataset),\n                'features': dataset[0].x.shape[1] if hasattr(dataset[0], 'x') else 'unknown'\n            },\n            'evaluation_time': time.time() - start_time,\n            'config': self.config.__dict__\n        }\n        \n        # Save results\n        self._save_results(evaluation_results, task.task_name)\n        \n        # Generate report\n        self._generate_report(evaluation_results, task.task_name)\n        \n        return evaluation_results\n    \n    def _cross_validate(\n        self,\n        model: BaseConnectomeModel,\n        dataset,\n        task: BaseConnectomeTask,\n        metrics: List[str]\n    ) -> Dict[str, Any]:\n        \"\"\"Perform cross-validation evaluation.\"\"\"\n        print(f\"Running {self.config.n_folds}-fold cross-validation...\")\n        \n        # Prepare stratification labels if needed\n        if self.config.stratify and task.task_type in ['binary_classification', 'multiclass_classification']:\n            y_labels = []\n            for i in range(len(dataset)):\n                data = dataset[i]\n                if hasattr(data, 'y') and data.y is not None:\n                    if torch.is_tensor(data.y):\n                        y_labels.append(int(data.y.item()))\n                    else:\n                        y_labels.append(int(data.y))\n                else:\n                    y_labels.append(0)  # Default label\n            \n            kfold = StratifiedKFold(\n                n_splits=self.config.n_folds,\n                shuffle=True,\n                random_state=self.config.random_state\n            )\n            splits = list(kfold.split(range(len(dataset)), y_labels))\n        else:\n            kfold = KFold(\n                n_splits=self.config.n_folds,\n                shuffle=True,\n                random_state=self.config.random_state\n            )\n            splits = list(kfold.split(range(len(dataset))))\n        \n        fold_results = []\n        all_predictions = []\n        all_targets = []\n        \n        for fold, (train_idx, val_idx) in enumerate(splits):\n            if self.config.verbose:\n                print(f\"  Fold {fold + 1}/{self.config.n_folds}\")\n            \n            # Create fold datasets\n            train_data = [dataset[i] for i in train_idx]\n            val_data = [dataset[i] for i in val_idx]\n            \n            # Clone and train model for this fold\n            fold_model = type(model)(**model.get_config() if hasattr(model, 'get_config') else {})\n            fold_model.load_state_dict(model.state_dict())\n            \n            # Quick training on fold (simplified)\n            fold_trainer = self._create_trainer(fold_model, task)\n            \n            # Create data loaders\n            from torch_geometric.loader import DataLoader\n            train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n            val_loader = DataLoader(val_data, batch_size=16, shuffle=False)\n            \n            # Train for a few epochs (simplified for benchmarking)\n            fold_trainer.fit(train_loader, val_loader)\n            \n            # Evaluate fold\n            fold_results_dict = fold_trainer.evaluate(val_loader)\n            fold_results.append(fold_results_dict)\n            \n            # Store predictions for later analysis\n            predictions, targets = self._get_predictions_targets(fold_model, val_data, task)\n            all_predictions.extend(predictions)\n            all_targets.extend(targets)\n        \n        # Compute cross-validation statistics\n        cv_stats = self._compute_cv_statistics(fold_results, metrics)\n        \n        # Compute overall metrics on all predictions\n        if all_predictions and all_targets:\n            overall_metrics = self._compute_metrics(\n                np.array(all_predictions),\n                np.array(all_targets),\n                task.task_type,\n                metrics\n            )\n            cv_stats['overall_metrics'] = overall_metrics\n        \n        return {\n            'fold_results': fold_results,\n            'cv_statistics': cv_stats,\n            'all_predictions': all_predictions if self.config.save_predictions else None,\n            'all_targets': all_targets if self.config.save_predictions else None\n        }\n    \n    def _holdout_evaluate(\n        self,\n        model: BaseConnectomeModel,\n        dataset,\n        task: BaseConnectomeTask,\n        metrics: List[str]\n    ) -> Dict[str, Any]:\n        \"\"\"Perform holdout evaluation.\"\"\"\n        if self.config.verbose:\n            print(\"Running holdout evaluation...\")\n        \n        # Split dataset\n        n_samples = len(dataset)\n        n_test = int(n_samples * self.config.test_size)\n        n_train = n_samples - n_test\n        \n        # Random split\n        np.random.seed(self.config.random_state)\n        indices = np.random.permutation(n_samples)\n        train_idx = indices[:n_train]\n        test_idx = indices[n_train:]\n        \n        # Create datasets\n        train_data = [dataset[i] for i in train_idx]\n        test_data = [dataset[i] for i in test_idx]\n        \n        # Train model\n        trainer = self._create_trainer(model, task)\n        \n        from torch_geometric.loader import DataLoader\n        train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n        test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n        \n        # Train model\n        trainer.fit(train_loader, test_loader)\n        \n        # Evaluate\n        test_results = trainer.evaluate(test_loader)\n        \n        # Get predictions for detailed analysis\n        predictions, targets = self._get_predictions_targets(model, test_data, task)\n        \n        # Compute additional metrics\n        detailed_metrics = self._compute_metrics(\n            np.array(predictions),\n            np.array(targets),\n            task.task_type,\n            metrics\n        )\n        \n        return {\n            'test_metrics': test_results,\n            'detailed_metrics': detailed_metrics,\n            'predictions': predictions if self.config.save_predictions else None,\n            'targets': targets if self.config.save_predictions else None,\n            'train_size': len(train_data),\n            'test_size': len(test_data)\n        }\n    \n    def _create_trainer(self, model: BaseConnectomeModel, task: BaseConnectomeTask):\n        \"\"\"Create trainer for benchmarking.\"\"\"\n        config = TrainingConfig(\n            batch_size=8,\n            learning_rate=1e-3,\n            max_epochs=20,  # Reduced for benchmarking\n            patience=5,\n            val_check_interval=1\n        )\n        \n        return AdvancedConnectomeTrainer(\n            model=model,\n            task=task,\n            config=config,\n            output_dir=self.output_dir / \"training_temp\",\n            use_wandb=False\n        )\n    \n    def _get_predictions_targets(self, model, dataset, task):\n        \"\"\"Get model predictions and targets.\"\"\"\n        model.eval()\n        predictions = []\n        targets = []\n        \n        with torch.no_grad():\n            for data in dataset:\n                # Get prediction\n                output = model(data)\n                if isinstance(output, dict):\n                    pred = output['predictions']\n                else:\n                    pred = output\n                \n                # Get target\n                target = task.prepare_targets([data])\n                target = task.normalize_targets(target)\n                \n                predictions.append(pred.cpu().numpy())\n                targets.append(target.cpu().numpy())\n        \n        return predictions, targets\n    \n    def _compute_cv_statistics(self, fold_results, metrics):\n        \"\"\"Compute cross-validation statistics.\"\"\"\n        cv_stats = {}\n        \n        # Collect metric values across folds\n        for metric in metrics:\n            values = []\n            for fold_result in fold_results:\n                if metric in fold_result:\n                    values.append(fold_result[metric])\n            \n            if values:\n                cv_stats[f\"{metric}_mean\"] = np.mean(values)\n                cv_stats[f\"{metric}_std\"] = np.std(values)\n                cv_stats[f\"{metric}_values\"] = values\n        \n        return cv_stats\n    \n    def _compute_metrics(\n        self,\n        predictions: np.ndarray,\n        targets: np.ndarray,\n        task_type: str,\n        metrics: List[str]\n    ) -> Dict[str, float]:\n        \"\"\"Compute evaluation metrics.\"\"\"\n        results = {}\n        \n        try:\n            if task_type == 'regression':\n                if 'mae' in metrics:\n                    results['mae'] = mean_absolute_error(targets, predictions)\n                if 'rmse' in metrics:\n                    results['rmse'] = np.sqrt(mean_squared_error(targets, predictions))\n                if 'r2' in metrics:\n                    results['r2'] = r2_score(targets, predictions)\n                if 'pearson_r' in metrics:\n                    corr, _ = pearsonr(targets.flatten(), predictions.flatten())\n                    results['pearson_r'] = corr if not np.isnan(corr) else 0.0\n                if 'spearman_r' in metrics:\n                    corr, _ = spearmanr(targets.flatten(), predictions.flatten())\n                    results['spearman_r'] = corr if not np.isnan(corr) else 0.0\n            \n            elif task_type in ['binary_classification', 'multiclass_classification']:\n                # Convert predictions to class labels if needed\n                if predictions.ndim > 1 and predictions.shape[1] > 1:\n                    pred_classes = np.argmax(predictions, axis=1)\n                    pred_probs = predictions\n                else:\n                    pred_classes = (predictions > 0.5).astype(int)\n                    pred_probs = predictions\n                \n                if targets.ndim > 1:\n                    target_classes = np.argmax(targets, axis=1)\n                else:\n                    target_classes = targets.astype(int)\n                \n                if 'accuracy' in metrics:\n                    results['accuracy'] = accuracy_score(target_classes, pred_classes)\n                if 'precision' in metrics:\n                    results['precision'] = precision_score(\n                        target_classes, pred_classes, average='weighted', zero_division=0\n                    )\n                if 'recall' in metrics:\n                    results['recall'] = recall_score(\n                        target_classes, pred_classes, average='weighted', zero_division=0\n                    )\n                if 'f1' in metrics:\n                    results['f1'] = f1_score(\n                        target_classes, pred_classes, average='weighted', zero_division=0\n                    )\n                if 'macro_f1' in metrics:\n                    results['macro_f1'] = f1_score(\n                        target_classes, pred_classes, average='macro', zero_division=0\n                    )\n                \n                # AUC metrics (only for binary or with probabilities)\n                if task_type == 'binary_classification' and pred_probs.ndim == 1:\n                    if 'auc' in metrics:\n                        try:\n                            results['auc'] = roc_auc_score(target_classes, pred_probs)\n                        except ValueError:\n                            results['auc'] = 0.5  # Random performance\n                    if 'ap' in metrics:\n                        try:\n                            results['ap'] = average_precision_score(target_classes, pred_probs)\n                        except ValueError:\n                            results['ap'] = 0.5\n        \n        except Exception as e:\n            print(f\"Error computing metrics: {e}\")\n            # Return default values\n            for metric in metrics:\n                results[metric] = 0.0\n        \n        return results\n    \n    def _save_results(self, results: Dict[str, Any], task_name: str):\n        \"\"\"Save benchmark results to file.\"\"\"\n        results_file = self.output_dir / f\"{task_name}_results.json\"\n        \n        # Convert numpy arrays to lists for JSON serialization\n        serializable_results = self._make_serializable(results)\n        \n        with open(results_file, 'w') as f:\n            json.dump(serializable_results, f, indent=2, default=str)\n        \n        if self.config.verbose:\n            print(f\"Results saved to {results_file}\")\n    \n    def _make_serializable(self, obj):\n        \"\"\"Make object JSON serializable.\"\"\"\n        if isinstance(obj, dict):\n            return {k: self._make_serializable(v) for k, v in obj.items()}\n        elif isinstance(obj, list):\n            return [self._make_serializable(v) for v in obj]\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        elif isinstance(obj, (np.int32, np.int64)):\n            return int(obj)\n        elif isinstance(obj, (np.float32, np.float64)):\n            return float(obj)\n        else:\n            return obj\n    \n    def _generate_report(self, results: Dict[str, Any], task_name: str):\n        \"\"\"Generate comprehensive benchmark report.\"\"\"\n        if self.config.verbose:\n            print(\"\\n=== BENCHMARK REPORT ===\")\n            print(f\"Task: {task_name}\")\n            print(f\"Model: {results['model_info']['name']}\")\n            print(f\"Dataset Size: {results['dataset_info']['size']}\")\n            print(f\"Model Parameters: {results['model_info']['parameters']:,}\")\n            \n            # Cross-validation results\n            if 'cross_validation' in results:\n                print(\"\\n--- Cross-Validation Results ---\")\n                cv_stats = results['cross_validation']['cv_statistics']\n                for key, value in cv_stats.items():\n                    if key.endswith('_mean'):\n                        metric = key.replace('_mean', '')\n                        std_key = f\"{metric}_std\"\n                        if std_key in cv_stats:\n                            print(f\"{metric.upper()}: {value:.4f} ± {cv_stats[std_key]:.4f}\")\n                        else:\n                            print(f\"{metric.upper()}: {value:.4f}\")\n            \n            # Holdout results\n            if 'holdout' in results:\n                print(\"\\n--- Holdout Results ---\")\n                holdout_metrics = results['holdout']['detailed_metrics']\n                for metric, value in holdout_metrics.items():\n                    print(f\"{metric.upper()}: {value:.4f}\")\n            \n            print(f\"\\nEvaluation Time: {results['evaluation_time']:.2f} seconds\")\n        \n        # Generate plots\n        self._generate_plots(results, task_name)\n    \n    def _generate_plots(self, results: Dict[str, Any], task_name: str):\n        \"\"\"Generate visualization plots for results.\"\"\"\n        try:\n            # Cross-validation metrics plot\n            if 'cross_validation' in results and 'cv_statistics' in results['cross_validation']:\n                self._plot_cv_results(results['cross_validation']['cv_statistics'], task_name)\n            \n            # Prediction plots if available\n            if 'holdout' in results and results['holdout'].get('predictions'):\n                self._plot_predictions(\n                    results['holdout']['predictions'],\n                    results['holdout']['targets'],\n                    task_name,\n                    results['task_info']['task_type']\n                )\n        \n        except Exception as e:\n            print(f\"Error generating plots: {e}\")\n    \n    def _plot_cv_results(self, cv_stats: Dict[str, Any], task_name: str):\n        \"\"\"Plot cross-validation results.\"\"\"\n        # Find metrics with values across folds\n        metrics_data = {}\n        for key, value in cv_stats.items():\n            if key.endswith('_values') and isinstance(value, list):\n                metric_name = key.replace('_values', '')\n                metrics_data[metric_name] = value\n        \n        if not metrics_data:\n            return\n        \n        # Create plot\n        n_metrics = len(metrics_data)\n        fig, axes = plt.subplots(1, min(n_metrics, 4), figsize=(4*min(n_metrics, 4), 4))\n        \n        if n_metrics == 1:\n            axes = [axes]\n        elif n_metrics > 4:\n            axes = axes[:4]\n            metrics_data = dict(list(metrics_data.items())[:4])\n        \n        for idx, (metric, values) in enumerate(metrics_data.items()):\n            ax = axes[idx] if n_metrics > 1 else axes[0]\n            \n            # Box plot of cross-validation results\n            ax.boxplot(values)\n            ax.set_title(f'{metric.upper()}\\nCross-Validation')\n            ax.set_ylabel('Score')\n            \n            # Add mean line\n            mean_val = np.mean(values)\n            ax.axhline(y=mean_val, color='r', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.3f}')\n            ax.legend()\n        \n        plt.suptitle(f'Cross-Validation Results: {task_name}')\n        plt.tight_layout()\n        \n        plot_path = self.output_dir / f'{task_name}_cv_results.png'\n        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n        plt.close()\n        \n        if self.config.verbose:\n            print(f\"Cross-validation plot saved to {plot_path}\")\n    \n    def _plot_predictions(\n        self,\n        predictions: List[np.ndarray],\n        targets: List[np.ndarray],\n        task_name: str,\n        task_type: str\n    ):\n        \"\"\"Plot predictions vs targets.\"\"\"\n        pred_array = np.concatenate(predictions) if isinstance(predictions[0], np.ndarray) else np.array(predictions)\n        target_array = np.concatenate(targets) if isinstance(targets[0], np.ndarray) else np.array(targets)\n        \n        if task_type == 'regression':\n            # Scatter plot for regression\n            plt.figure(figsize=(8, 6))\n            plt.scatter(target_array, pred_array, alpha=0.6, s=20)\n            \n            # Perfect prediction line\n            min_val = min(target_array.min(), pred_array.min())\n            max_val = max(target_array.max(), pred_array.max())\n            plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.7, label='Perfect Prediction')\n            \n            plt.xlabel('True Values')\n            plt.ylabel('Predictions')\n            plt.title(f'Predictions vs True Values: {task_name}')\n            plt.legend()\n            \n            # Add correlation info\n            corr = np.corrcoef(target_array.flatten(), pred_array.flatten())[0, 1]\n            plt.text(0.05, 0.95, f'Correlation: {corr:.3f}', transform=plt.gca().transAxes,\n                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n        \n        else:\n            # Classification: confusion matrix\n            if pred_array.ndim > 1:\n                pred_classes = np.argmax(pred_array, axis=1)\n            else:\n                pred_classes = (pred_array > 0.5).astype(int)\n            \n            if target_array.ndim > 1:\n                target_classes = np.argmax(target_array, axis=1)\n            else:\n                target_classes = target_array.astype(int)\n            \n            cm = confusion_matrix(target_classes, pred_classes)\n            \n            plt.figure(figsize=(8, 6))\n            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n            plt.xlabel('Predicted Labels')\n            plt.ylabel('True Labels')\n            plt.title(f'Confusion Matrix: {task_name}')\n        \n        plt.tight_layout()\n        plot_path = self.output_dir / f'{task_name}_predictions.png'\n        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n        plt.close()\n        \n        if self.config.verbose:\n            print(f\"Predictions plot saved to {plot_path}\")\n\n\n# Available benchmark tasks\nclass AgePredictionBenchmark(SubjectBenchmark):\n    \"\"\"Age prediction benchmark.\"\"\"\n    \n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.task_name = \"age_prediction\"\n        self.metrics = ['mae', 'rmse', 'r2', 'pearson_r']\n\n\nclass SexClassificationBenchmark(SubjectBenchmark):\n    \"\"\"Sex classification benchmark.\"\"\"\n    \n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.task_name = \"sex_classification\"\n        self.metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n\n\nclass CognitiveScoreBenchmark(SubjectBenchmark):\n    \"\"\"Cognitive score prediction benchmark.\"\"\"\n    \n    def __init__(self, cognitive_measure: str = \"fluid_intelligence\", **kwargs):\n        super().__init__(**kwargs)\n        self.task_name = f\"cognitive_{cognitive_measure}\"\n        self.cognitive_measure = cognitive_measure\n        self.metrics = ['mae', 'rmse', 'r2', 'pearson_r', 'spearman_r']